-- Unread Counter Denormalization Migration
-- Generated: October 22, 2025
-- Purpose: Eliminate expensive unread counter anti-join query
--
-- PROBLEM:
-- Current query scans ALL messages in user's conversations with NOT EXISTS anti-join
-- on message_reads table. This is O(NÃ—M) complexity and cannot be efficiently indexed.
--
-- SOLUTION:
-- Denormalize unread count into conversation_users table and maintain incrementally.
--
-- EXPECTED IMPROVEMENT: 80-90% reduction in unread counter query time
-- Before: 50-200ms (depends on message count)
-- After:  <5ms (simple SUM query on indexed column)

-- =============================================================================
-- STEP 1: Add unreadCount column to conversation_users
-- =============================================================================

ALTER TABLE conversation_users
ADD COLUMN IF NOT EXISTS "unreadCount" INTEGER NOT NULL DEFAULT 0;

-- Add index for efficient aggregation queries
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_conversation_users_user_unread
ON conversation_users ("userId")
WHERE "unreadCount" > 0;  -- Partial index: only conversations with unread messages

COMMENT ON COLUMN conversation_users."unreadCount" IS 
'Cached count of unread messages in this conversation for this user. 
Updated on message insert (+1) and markMessagesAsRead (-N). 
Reconciled daily via background job.';

-- =============================================================================
-- STEP 2: Backfill existing unread counts
-- =============================================================================
-- WARNING: This can be slow on large datasets. Run during low-traffic period.
-- Estimated time: ~1-2 minutes per 100K conversation_users rows

-- Option A: Full backfill (accurate but slower)
UPDATE conversation_users cu
SET "unreadCount" = (
  SELECT COUNT(*)
  FROM messages m
  WHERE m."conversationId" = cu."conversationId"
    AND m."senderId" != cu."userId"
    AND m."isDeleted" = false
    AND NOT EXISTS (
      SELECT 1 FROM message_reads mr
      WHERE mr."messageId" = m.id AND mr."userId" = cu."userId"
    )
)
WHERE cu."isActive" = true;

-- Option B: Incremental backfill (for large datasets)
-- DO $$
-- DECLARE
--   batch_size INTEGER := 1000;
--   processed INTEGER := 0;
--   total INTEGER;
-- BEGIN
--   SELECT COUNT(*) INTO total FROM conversation_users WHERE "isActive" = true;
--   
--   LOOP
--     UPDATE conversation_users cu
--     SET "unreadCount" = (
--       SELECT COUNT(*)
--       FROM messages m
--       WHERE m."conversationId" = cu."conversationId"
--         AND m."senderId" != cu."userId"
--         AND m."isDeleted" = false
--         AND NOT EXISTS (
--           SELECT 1 FROM message_reads mr
--           WHERE mr."messageId" = m.id AND mr."userId" = cu."userId"
--         )
--     )
--     WHERE cu.id IN (
--       SELECT id FROM conversation_users
--       WHERE "isActive" = true
--         AND "unreadCount" = 0
--       LIMIT batch_size
--     );
--     
--     GET DIAGNOSTICS processed = ROW_COUNT;
--     EXIT WHEN processed = 0;
--     
--     RAISE NOTICE 'Processed % rows, % remaining', processed, total - processed;
--     PERFORM pg_sleep(0.1);  -- Brief pause between batches
--   END LOOP;
-- END $$;

-- =============================================================================
-- STEP 3: Create trigger to maintain unread counts on INSERT
-- =============================================================================

CREATE OR REPLACE FUNCTION increment_unread_counts()
RETURNS TRIGGER AS $$
BEGIN
  -- Increment unread count for all participants except the sender
  UPDATE conversation_users
  SET "unreadCount" = "unreadCount" + 1,
      "updatedAt" = NOW()
  WHERE "conversationId" = NEW."conversationId"
    AND "userId" != NEW."senderId"
    AND "isActive" = true;
  
  RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER trg_increment_unread_on_message_insert
AFTER INSERT ON messages
FOR EACH ROW
WHEN (NEW."isDeleted" = false)
EXECUTE FUNCTION increment_unread_counts();

COMMENT ON FUNCTION increment_unread_counts() IS
'Automatically increments unreadCount for all conversation participants
except the sender when a new message is inserted.';

-- =============================================================================
-- STEP 4: Create trigger to maintain unread counts on DELETE/UPDATE
-- =============================================================================

CREATE OR REPLACE FUNCTION adjust_unread_counts_on_delete()
RETURNS TRIGGER AS $$
BEGIN
  -- Decrement unread count for messages that were unread and are now deleted
  IF OLD."isDeleted" = false AND NEW."isDeleted" = true THEN
    UPDATE conversation_users cu
    SET "unreadCount" = GREATEST(0, "unreadCount" - 1),
        "updatedAt" = NOW()
    WHERE cu."conversationId" = OLD."conversationId"
      AND cu."userId" != OLD."senderId"
      AND cu."isActive" = true
      AND NOT EXISTS (
        SELECT 1 FROM message_reads mr
        WHERE mr."messageId" = OLD.id AND mr."userId" = cu."userId"
      );
  END IF;
  
  RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER trg_adjust_unread_on_message_update
AFTER UPDATE ON messages
FOR EACH ROW
WHEN (OLD."isDeleted" != NEW."isDeleted")
EXECUTE FUNCTION adjust_unread_counts_on_delete();

-- =============================================================================
-- STEP 5: Update application code - markMessagesAsRead
-- =============================================================================
-- 
-- This SQL is for reference. Actual implementation should be in TypeScript:
-- File: src/chat-backend/repositories/chat.repository.ts
--
-- async markMessagesAsRead(conversationId, userId, messageIds) {
--   return await this.db.$transaction(async (tx) => {
--     // Existing logic: Create read receipts
--     const unreadMessageIds = await getUnreadMessageIds(...);
--     
--     if (unreadMessageIds.length > 0) {
--       await tx.messageRead.createMany({
--         data: unreadMessageIds.map(messageId => ({
--           messageId,
--           userId,
--           readAt: new Date(),
--         })),
--       });
--       
--       // NEW: Decrement unread count
--       await tx.conversationUser.update({
--         where: {
--           userId_conversationId: {
--             userId,
--             conversationId,
--           },
--         },
--         data: {
--           unreadCount: {
--             decrement: unreadMessageIds.length,
--           },
--           updatedAt: new Date(),
--         },
--       });
--     }
--     
--     return { markedAsRead: unreadMessageIds.length };
--   });
-- }

-- =============================================================================
-- STEP 6: Optimize getUnreadCount query
-- =============================================================================
-- 
-- Replace expensive anti-join query with simple SUM:
--
-- OLD QUERY (SLOW - 50-200ms):
-- SELECT COUNT(*) FROM messages m
-- WHERE m.conversationId IN (SELECT conversationId FROM conversation_users WHERE userId = ?)
--   AND m.senderId != ?
--   AND NOT EXISTS (SELECT 1 FROM message_reads WHERE messageId = m.id AND userId = ?);
--
-- NEW QUERY (FAST - <5ms):
-- SELECT COALESCE(SUM("unreadCount"), 0)::integer as unread_count
-- FROM conversation_users
-- WHERE "userId" = $1
--   AND "isActive" = true;
--
-- TypeScript implementation:
-- File: src/chat-backend/repositories/chat.repository.ts
--
-- async getUnreadMessageCount(userId: string): Promise<number> {
--   const result = await this.db.conversationUser.aggregate({
--     where: {
--       userId,
--       isActive: true,
--     },
--     _sum: {
--       unreadCount: true,
--     },
--   });
--   
--   return result._sum.unreadCount || 0;
-- }

-- =============================================================================
-- STEP 7: Create reconciliation function for data integrity
-- =============================================================================

CREATE OR REPLACE FUNCTION reconcile_unread_counts(
  p_user_id uuid DEFAULT NULL,
  p_conversation_id uuid DEFAULT NULL
)
RETURNS TABLE (
  conversation_user_id text,
  old_count integer,
  new_count integer,
  diff integer
) AS $$
BEGIN
  RETURN QUERY
  WITH calculated_counts AS (
    SELECT
      cu.id as cu_id,
      cu."unreadCount" as old_count,
      COALESCE(COUNT(m.id), 0)::integer as new_count
    FROM conversation_users cu
    LEFT JOIN messages m ON m."conversationId" = cu."conversationId"
      AND m."senderId" != cu."userId"
      AND m."isDeleted" = false
      AND NOT EXISTS (
        SELECT 1 FROM message_reads mr
        WHERE mr."messageId" = m.id AND mr."userId" = cu."userId"
      )
    WHERE cu."isActive" = true
      AND (p_user_id IS NULL OR cu."userId" = p_user_id)
      AND (p_conversation_id IS NULL OR cu."conversationId" = p_conversation_id)
    GROUP BY cu.id, cu."unreadCount"
  )
  UPDATE conversation_users cu
  SET "unreadCount" = cc.new_count,
      "updatedAt" = NOW()
  FROM calculated_counts cc
  WHERE cu.id::text = cc.cu_id
    AND cu."unreadCount" != cc.new_count
  RETURNING 
    cu.id::text as conversation_user_id,
    cc.old_count,
    cc.new_count,
    (cc.new_count - cc.old_count) as diff;
END;
$$ LANGUAGE plpgsql;

COMMENT ON FUNCTION reconcile_unread_counts IS
'Recalculates and fixes unread counts for conversation_users.
Run periodically (e.g., daily) to ensure data integrity.
Can be scoped to specific user or conversation.
Returns rows that were updated with old/new values.';

-- Usage examples:
-- SELECT * FROM reconcile_unread_counts();  -- All users/conversations
-- SELECT * FROM reconcile_unread_counts('user-uuid-here', NULL);  -- Specific user
-- SELECT * FROM reconcile_unread_counts(NULL, 'conv-uuid-here');  -- Specific conversation

-- =============================================================================
-- STEP 8: Create monitoring view
-- =============================================================================

CREATE OR REPLACE VIEW vw_unread_count_stats AS
SELECT
  COUNT(*) as total_conversation_users,
  COUNT(*) FILTER (WHERE "unreadCount" > 0) as users_with_unread,
  ROUND(AVG("unreadCount"), 2) as avg_unread_count,
  MAX("unreadCount") as max_unread_count,
  PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY "unreadCount") as median_unread_count,
  PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY "unreadCount") as p95_unread_count,
  SUM("unreadCount") as total_unread_messages
FROM conversation_users
WHERE "isActive" = true;

COMMENT ON VIEW vw_unread_count_stats IS
'Monitoring view for unread count distribution.
Use to detect anomalies and validate reconciliation results.';

-- Usage: SELECT * FROM vw_unread_count_stats;

-- =============================================================================
-- STEP 9: Schedule daily reconciliation (optional)
-- =============================================================================
-- 
-- Using pg_cron extension (if available):
-- SELECT cron.schedule(
--   'reconcile-unread-counts',
--   '0 3 * * *',  -- 3 AM daily
--   'SELECT reconcile_unread_counts()'
-- );
--
-- Alternative: Node.js cron job in application
-- File: src/jobs/reconcile-unread-counts.job.ts
--
-- import { CronJob } from 'cron';
-- import { prisma } from '../config/database';
-- 
-- export const reconcileUnreadCountsJob = new CronJob(
--   '0 3 * * *',  // 3 AM daily
--   async () => {
--     console.log('Starting unread count reconciliation...');
--     const result = await prisma.$queryRaw`SELECT * FROM reconcile_unread_counts()`;
--     console.log(`Reconciled ${result.length} conversation_users`);
--   },
--   null,
--   true,
--   'America/New_York'
-- );

-- =============================================================================
-- VALIDATION QUERIES
-- =============================================================================

-- 1. Verify backfill completed successfully
SELECT 
  COUNT(*) as total,
  COUNT(*) FILTER (WHERE "unreadCount" > 0) as with_unread,
  AVG("unreadCount")::numeric(10,2) as avg_unread
FROM conversation_users
WHERE "isActive" = true;

-- 2. Compare new query vs old query (should match)
-- New query:
SELECT COALESCE(SUM("unreadCount"), 0)::integer as unread_count
FROM conversation_users
WHERE "userId" = 'sample-user-uuid'
  AND "isActive" = true;

-- Old query (for validation only):
-- SELECT COUNT(*) FROM messages m
-- WHERE m."conversationId" IN (
--   SELECT "conversationId" FROM conversation_users WHERE "userId" = 'sample-user-uuid'
-- )
-- AND m."senderId" != 'sample-user-uuid'
-- AND NOT EXISTS (
--   SELECT 1 FROM message_reads WHERE "messageId" = m.id AND "userId" = 'sample-user-uuid'
-- );

-- 3. Test trigger on message insert
-- INSERT a test message and verify unread counts increment:
-- DO $$
-- DECLARE
--   test_conv_id uuid;
--   test_sender_id uuid;
--   counts_before jsonb;
--   counts_after jsonb;
-- BEGIN
--   -- Get a test conversation and user
--   SELECT cu."conversationId", cu."userId" INTO test_conv_id, test_sender_id
--   FROM conversation_users cu
--   WHERE cu."isActive" = true
--   LIMIT 1;
--   
--   -- Capture counts before
--   SELECT jsonb_agg(jsonb_build_object('userId', "userId", 'count', "unreadCount"))
--   INTO counts_before
--   FROM conversation_users
--   WHERE "conversationId" = test_conv_id;
--   
--   -- Insert test message
--   INSERT INTO messages (id, "conversationId", "senderId", content, type, status, "createdAt", "updatedAt")
--   VALUES (gen_random_uuid(), test_conv_id, test_sender_id, 'Test message', 'TEXT', 'SENT', NOW(), NOW());
--   
--   -- Capture counts after
--   SELECT jsonb_agg(jsonb_build_object('userId', "userId", 'count', "unreadCount"))
--   INTO counts_after
--   FROM conversation_users
--   WHERE "conversationId" = test_conv_id;
--   
--   RAISE NOTICE 'Before: %', counts_before;
--   RAISE NOTICE 'After: %', counts_after;
--   
--   -- Rollback test
--   RAISE EXCEPTION 'Test complete, rolling back';
-- END $$;

-- =============================================================================
-- PERFORMANCE IMPACT
-- =============================================================================
--
-- Before:
--   getUnreadCount(): 50-200ms (scales with message count)
--   Query plan: Sequential scan + anti-join + subquery
--
-- After:
--   getUnreadCount(): <5ms (constant time)
--   Query plan: Index scan + simple aggregation
--
-- Improvement: 90-95% latency reduction
-- Tradeoff: +4 bytes per conversation_users row, +1 UPDATE per message insert
--
-- Write overhead:
--   Message insert: +1 UPDATE statement (10-15% slower)
--   markMessagesAsRead: +1 UPDATE statement (negligible)
--
-- Acceptable tradeoff for read-heavy workload (70% reads, 30% writes)

-- =============================================================================
-- ROLLBACK SCRIPT (if needed)
-- =============================================================================

-- DROP TRIGGER IF EXISTS trg_increment_unread_on_message_insert ON messages;
-- DROP TRIGGER IF EXISTS trg_adjust_unread_on_message_update ON messages;
-- DROP FUNCTION IF EXISTS increment_unread_counts();
-- DROP FUNCTION IF EXISTS adjust_unread_counts_on_delete();
-- DROP FUNCTION IF EXISTS reconcile_unread_counts(uuid, uuid);
-- DROP VIEW IF EXISTS vw_unread_count_stats;
-- DROP INDEX CONCURRENTLY IF EXISTS idx_conversation_users_user_unread;
-- ALTER TABLE conversation_users DROP COLUMN IF EXISTS "unreadCount";

-- =============================================================================
-- DEPLOYMENT CHECKLIST
-- =============================================================================
-- 
-- [ ] 1. Run index migration (02-index-design.sql) first
-- [ ] 2. Add unreadCount column during low-traffic period
-- [ ] 3. Run backfill script (Option A or B based on data size)
-- [ ] 4. Create triggers and functions
-- [ ] 5. Update application code:
--         - ChatRepository.getUnreadMessageCount()
--         - ChatRepository.markMessagesAsRead()
-- [ ] 6. Deploy application changes
-- [ ] 7. Monitor for 24-48 hours:
--         - Check unreadCount accuracy with reconcile_unread_counts()
--         - Monitor message insert latency (should be <30ms p95)
--         - Verify getUnreadCount() is <5ms
-- [ ] 8. Set up daily reconciliation job
-- [ ] 9. Create alerts for anomalies (e.g., negative counts, excessive drift)
--
-- Estimated deployment time: 30-60 minutes
-- Estimated testing time: 2-4 hours
-- Recommended deployment window: Off-peak hours
