# Performance Environment Provisioning & Dataset Validation

## Overview

Complete toolkit for **DevOps/SRE environment provisioning** and **Data QA validation** before Phase 2 performance testing.

## ðŸŽ¯ Quick Start

### Step 1: Provision Performance Database

```bash
cd /Users/harishraghave/Desktop/colleging/college-id-signup-1/scripts/infra

# Provision dev environment (5M messages)
./provision-perf-db.sh --env dev

# Or staging environment (100M messages)
./provision-perf-db.sh --env staging

# Or perf environment (300M+ messages)
./provision-perf-db.sh --env perf
```

**Output**:

- Isolated database with least-privilege user
- pg_stat_statements enabled for query monitoring
- Autovacuum tuned for high-churn workloads
- Credentials saved to `.secrets/.env.{env}`

---

### Step 2: Generate Synthetic Data

```bash
# Load environment
source .secrets/.env.dev

# Generate data
cd scripts/synthetic-data
NODE_OPTIONS="--max-old-space-size=4096" npx ts-node quick-generator.ts
```

---

### Step 3: Validate Dataset Quality

```bash
# Run comprehensive validation
cd scripts/synthetic-data
npx ts-node validate-dataset-quality.ts --output quality-report-20251022.json
```

**Output**:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  OVERALL VERDICT: PASS                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Tests passed: 8/8
Ready for Phase 2: YES âœ“

RECOMMENDATIONS:
  âœ“ Dataset passes all quality checks
  âœ“ Heavy rooms present - will activate hot indexes and caching
  âœ“ Power-law tail realistic - queries will exhibit production-like skew
  âœ“ APPROVED for Phase 2 performance testing
```

---

## ðŸ› ï¸ Tool 1: Performance Database Provisioning

### `provision-perf-db.sh`

**Purpose**: Create isolated, monitored, safe environment for large-scale data tests.

**Features**:

- âœ… **Isolation**: Separate database with firewall rules and dedicated credentials
- âœ… **Observability**: pg_stat_statements enabled, slow query logging, autovacuum logging
- âœ… **Safety**: Least-privilege user (no superuser), disk space validation
- âœ… **Maintenance**: Autovacuum tuned for high-churn tables, increased work_mem

**Usage**:

```bash
./provision-perf-db.sh --env [dev|staging|perf] [--host HOST] [--port PORT]
```

**Environment Bands**:

| Band        | Messages | Users | Work Mem | Shared Buffers | Purpose                            |
| ----------- | -------- | ----- | -------- | -------------- | ---------------------------------- |
| **dev**     | 5M       | 5K    | 64MB     | 256MB          | Local development, quick iteration |
| **staging** | 100M     | 150K  | 128MB    | 1GB            | Pre-production validation          |
| **perf**    | 300M+    | 250K  | 256MB    | 2GB            | Load testing, capacity planning    |

**Security**:

- Generates secure random password (32 chars)
- Least-privilege user (no superuser, no CREATEDB)
- Credentials saved to gitignored `.secrets/` directory
- Connection helper scripts generated

**Observability**:

- `pg_stat_statements` enabled (track slow queries)
- `log_min_duration_statement = 1000ms` (log slow queries)
- `log_autovacuum_min_duration = 0` (log all vacuums)
- Monitoring queries generated in `.secrets/monitor-{env}.sql`

**Maintenance**:

- `autovacuum_vacuum_scale_factor = 0.05` (more frequent vacuums)
- `autovacuum_analyze_scale_factor = 0.02` (more frequent analyzes)
- `autovacuum_vacuum_cost_limit = 2000` (faster vacuums)
- `maintenance_work_mem` scaled per environment

**Output Files**:

```
.secrets/
â”œâ”€â”€ .env.dev                    # Environment variables (DATABASE_URL, credentials)
â”œâ”€â”€ connect-dev.sh              # Quick connect script
â””â”€â”€ monitor-dev.sql             # Monitoring queries
```

**Example Output**:

```bash
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  Performance Database Provisioning - DEV Environment      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Configuration:
  Database: chat_perf_dev
  User: perf_dev_user
  Host: localhost:5432
  Target Messages: 5,000,000
  Target Users: 5,000

[1/8] Checking PostgreSQL connectivity...
âœ“ Connected to PostgreSQL

[2/8] Validating disk space...
âœ“ Disk space sufficient (50GB available, ~10GB needed)

[3/8] Checking required extensions...
âœ“ Extensions configured

[4/8] Creating performance database...
âœ“ Database 'chat_perf_dev' created

[5/8] Creating dedicated user with least privilege...
âœ“ User 'perf_dev_user' created with least privilege

[6/8] Running Prisma migrations...
âœ“ Migrations applied

[7/8] Enabling query monitoring...
âœ“ Query monitoring enabled

[8/8] Writing configuration files...
âœ“ Credentials written to .secrets/.env.dev
âœ“ Monitoring queries written to .secrets/monitor-dev.sql

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  âœ“ Provisioning Complete                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Quick Start:
  # Load environment variables
  source .secrets/.env.dev

  # Connect to database
  .secrets/connect-dev.sh

  # Generate synthetic data
  cd scripts/synthetic-data
  NODE_OPTIONS="--max-old-space-size=4096" npx ts-node quick-generator.ts
```

**Monitoring Queries** (`.secrets/monitor-dev.sql`):

```sql
-- Top slow queries
SELECT query, calls, mean_exec_time, total_exec_time
FROM pg_stat_statements
ORDER BY mean_exec_time DESC
LIMIT 20;

-- Table sizes
SELECT tablename, pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename))
FROM pg_tables
WHERE schemaname = 'public'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;

-- Index usage
SELECT tablename, indexname, idx_scan, pg_size_pretty(pg_relation_size(indexrelid))
FROM pg_stat_user_indexes
ORDER BY idx_scan ASC;

-- Autovacuum activity
SELECT relname, last_vacuum, last_autovacuum, vacuum_count, autovacuum_count
FROM pg_stat_user_tables
ORDER BY autovacuum_count DESC;
```

---

## ðŸ” Tool 2: Dataset Quality Validation

### `validate-dataset-quality.ts`

**Purpose**: Validate synthetic dataset exhibits realistic distributions BEFORE Phase 2 testing.

**Validation Suite**:

1. **Heavy Room Analysis**
   - Top 1% of conversations should contain 15%+ of messages
   - Top 5% of conversations should contain 30%+ of messages
   - Ensures hot indexes and caches will be activated

2. **Power-Law Tail**
   - p99/p50 ratio should be >10x
   - Validates realistic skew (some huge rooms, many small ones)
   - Histogram buckets: 1-10, 11-50, 51-100, 101-500, 501-1K, 1K-5K, 5K+

3. **Type Distributions**
   - Conversation types (DIRECT_MESSAGE vs GROUP_CHAT)
   - Message types (TEXT vs IMAGE/FILE/AUDIO/VIDEO)
   - Should have reasonable variety (not 100% of one type)

4. **Temporal Patterns**
   - Hourly distribution (24 buckets)
   - Diurnal variation (peak/min ratio should be >1.5x)
   - Ensures temporal indexing strategies are tested

5. **Data Integrity**
   - **PII Safety**: No real email addresses (all syn*\* or synthetic*\*)
   - **Referential Integrity**: No orphaned messages, conversation_users, attachments
   - **Timestamp Ordering**: Messages created in chronological order
   - **Data Completeness**: No NULL foreign keys

**Usage**:

```bash
npx ts-node validate-dataset-quality.ts --output quality-report-20251022.json
```

**Output Format**:

```json
{
  "validation_timestamp": "2025-10-22T18:30:00Z",
  "dataset_summary": {
    "users": 5000,
    "conversations": 8000,
    "messages": 500000,
    "attachments": 15000
  },
  "distribution_analysis": {
    "heavy_rooms": {
      "top_1_percent_conversations": 80,
      "top_1_percent_message_share": 0.182,
      "top_5_percent_message_share": 0.345,
      "verdict": "PASS"
    },
    "power_law_tail": {
      "p50": 45,
      "p90": 485,
      "p99": 5150,
      "max": 12000,
      "tail_ratio_p99_p50": 114.4,
      "verdict": "PASS"
    }
  },
  "type_distributions": {
    "conversations": [
      { "type": "DIRECT_MESSAGE", "count": 5600, "percentage": 70.0 },
      { "type": "GROUP_CHAT", "count": 2400, "percentage": 30.0 }
    ],
    "messages": [
      { "type": "TEXT", "count": 425000, "percentage": 85.0 },
      { "type": "IMAGE", "count": 50000, "percentage": 10.0 }
    ],
    "verdict": "PASS"
  },
  "temporal_analysis": {
    "peak_hours": [9, 10, 11, 18, 19, 20],
    "diurnal_variation": 2.3,
    "verdict": "PASS"
  },
  "integrity_checks": {
    "pii_safety": {
      "synthetic_emails_only": true,
      "real_email_count": 0,
      "verdict": "PASS"
    },
    "referential_integrity": {
      "orphaned_messages": 0,
      "orphaned_conversation_users": 0,
      "verdict": "PASS"
    }
  },
  "overall_assessment": {
    "verdict": "PASS",
    "passed_tests": 8,
    "total_tests": 8,
    "ready_for_phase2": true
  },
  "recommendations": [
    "âœ“ Dataset passes all quality checks",
    "âœ“ Heavy rooms present - will activate hot indexes and caching",
    "âœ“ APPROVED for Phase 2 performance testing"
  ]
}
```

**Test Criteria**:

| Test                  | Pass Condition              | Rationale                               |
| --------------------- | --------------------------- | --------------------------------------- |
| Heavy rooms           | Top 1% has 15%+ of messages | Stresses hot paths, activates caches    |
| Power-law tail        | p99/p50 ratio > 10x         | Realistic skew, validates index choices |
| Type variety          | Multiple types present      | Tests all code paths                    |
| Diurnal variation     | Peak/min ratio > 1.5x       | Validates temporal indexing             |
| PII safety            | 0 real emails               | CRITICAL: Privacy compliance            |
| Referential integrity | 0 orphaned records          | CRITICAL: Data consistency              |
| Timestamp ordering    | 0 out-of-order              | Query correctness                       |
| Data completeness     | 0 NULL FKs                  | Query correctness                       |

---

## ðŸ”„ Complete Workflow

### Step 1: Provision Database

```bash
cd scripts/infra
./provision-perf-db.sh --env dev
```

**Outputs**:

- `.secrets/.env.dev` with DATABASE_URL
- `.secrets/connect-dev.sh` for quick connection
- `.secrets/monitor-dev.sql` for performance queries

---

### Step 2: Generate Synthetic Data

```bash
source .secrets/.env.dev
cd scripts/synthetic-data
NODE_OPTIONS="--max-old-space-size=4096" npx ts-node quick-generator.ts
```

**Duration**: ~10 minutes for 500K messages

---

### Step 3: Validate Dataset Quality

```bash
npx ts-node validate-dataset-quality.ts --output quality-report-dev.json
```

**Output**:

```
[1/6] Analyzing heavy room distribution...
     Top 1%: 80 convos with 18.2% of messages
     Top 5%: 400 convos with 34.5% of messages
     Verdict: PASS

[2/6] Analyzing power-law tail...
     Percentiles: p50=45, p75=120, p90=485, p95=1200, p99=5150, max=12000
     Tail ratio (p99/p50): 114.4x
     Verdict: PASS

[3/6] Validating type distributions...
     Conversation types:
       DIRECT_MESSAGE: 5600 (70.0%)
       GROUP_CHAT: 2400 (30.0%)
     Message types:
       TEXT: 425000 (85.0%)
       IMAGE: 50000 (10.0%)
     Verdict: PASS

[4/6] Analyzing temporal patterns...
     Peak hours: 9, 10, 11, 18, 19, 20
     Diurnal variation: 2.30x (max/min)
     Verdict: PASS

[5/6] Running integrity checks...
     PII Safety: 0 real emails (PASS)
     Referential integrity: 0 orphaned records (PASS)
     Timestamp ordering: 0 out-of-order (PASS)
     Data completeness: 0 null FKs (PASS)

[6/6] Generating final assessment...

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  OVERALL VERDICT: PASS                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Tests passed: 8/8
Ready for Phase 2: YES âœ“

RECOMMENDATIONS:
  âœ“ Dataset passes all quality checks
  âœ“ Heavy rooms present - will activate hot indexes and caching
  âœ“ Power-law tail realistic - queries will exhibit production-like skew
  âœ“ APPROVED for Phase 2 performance testing

âœ“ Report written to: quality-report-dev.json
```

---

### Step 4: Proceed with Phase 2 Testing

```bash
# Capture pre-optimization baseline
cd scripts/synthetic-data
npx ts-node verify-index-performance.ts --pre-optimization

# Apply Phase 2 indexes
psql $DATABASE_URL -f docs/database/indexes/phase-2-indexes.sql

# Capture post-optimization baseline
npx ts-node verify-index-performance.ts --post-optimization

# Compare results
npx ts-node verify-index-performance.ts --compare pre.json post.json
```

---

## ðŸ“Š Monitoring & Observability

### View Query Statistics

```bash
source .secrets/.env.dev
.secrets/connect-dev.sh -f .secrets/monitor-dev.sql
```

### Top Slow Queries

```sql
SELECT
  substring(query, 1, 60) as query_snippet,
  calls,
  round(mean_exec_time::numeric, 2) as avg_ms,
  round(total_exec_time::numeric, 2) as total_ms
FROM pg_stat_statements
ORDER BY mean_exec_time DESC
LIMIT 10;
```

### Table & Index Sizes

```sql
SELECT
  schemaname || '.' || tablename as table_name,
  pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as total_size,
  pg_size_pretty(pg_relation_size(schemaname||'.'||tablename)) as table_size,
  pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename) - pg_relation_size(schemaname||'.'||tablename)) as index_size
FROM pg_tables
WHERE schemaname = 'public'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;
```

### Autovacuum Activity

```sql
SELECT
  schemaname || '.' || relname as table_name,
  last_vacuum,
  last_autovacuum,
  vacuum_count,
  autovacuum_count,
  n_tup_ins as inserts,
  n_tup_upd as updates,
  n_tup_del as deletes
FROM pg_stat_user_tables
ORDER BY n_tup_ins + n_tup_upd + n_tup_del DESC;
```

---

## ðŸ”’ Security & Safety

### Isolation

- âœ… Separate database per environment (chat_perf_dev, chat_perf_staging, chat_perf)
- âœ… Dedicated users with least privilege (no superuser)
- âœ… Separate credentials from production
- âœ… Gitignored `.secrets/` directory for credentials

### Observability

- âœ… pg_stat_statements enabled (query tracking)
- âœ… Slow query logging (>1000ms)
- âœ… Autovacuum logging (all operations)
- âœ… Monitoring queries generated

### Maintenance

- âœ… Autovacuum tuned for high-churn (5% threshold)
- âœ… Increased autovacuum_cost_limit (faster vacuums)
- âœ… work_mem scaled per environment
- âœ… Disk space validated before operations

### Data Integrity

- âœ… PII safety check (no real emails)
- âœ… Referential integrity validation
- âœ… Timestamp ordering validation
- âœ… Data completeness validation

---

## ðŸ› Troubleshooting

### Error: "Insufficient disk space"

**Problem**: Not enough space for target dataset.

**Solution**:

- Free up space or use smaller environment (dev instead of staging)
- Estimate: ~500 bytes per message + 3x for indexes

---

### Error: "pg_stat_statements extension not available"

**Problem**: Extension not installed or shared_preload_libraries not configured.

**Solution**:

```sql
-- As superuser
ALTER SYSTEM SET shared_preload_libraries = 'pg_stat_statements';
-- Restart PostgreSQL
-- Then run provision script again
```

---

### Validation fails: "Insufficient heavy rooms"

**Problem**: Distribution too uniform, won't stress hot paths.

**Solution**:

- Decrease power-law alpha in generator (e.g., 1.8 â†’ 1.6)
- Regenerate dataset
- Re-validate

---

### Validation fails: "PII detected"

**Problem**: Real email addresses in dataset.

**Solution**:

- **CRITICAL**: Delete dataset immediately
- Ensure generator uses only `syn_` or `synthetic_` prefixes
- Regenerate dataset
- Re-validate

---

## ðŸ“ File Structure

```
scripts/
â”œâ”€â”€ infra/
â”‚   â””â”€â”€ provision-perf-db.sh              # Database provisioning script
â”œâ”€â”€ synthetic-data/
â”‚   â”œâ”€â”€ quick-generator.ts                 # Synthetic data generator (existing)
â”‚   â””â”€â”€ validate-dataset-quality.ts        # Dataset quality validation (NEW)
.secrets/                                   # Gitignored credentials
â”œâ”€â”€ .env.dev                                # Environment variables
â”œâ”€â”€ connect-dev.sh                          # Quick connect script
â””â”€â”€ monitor-dev.sql                         # Monitoring queries
```

---

## âœ… Success Criteria

### Provisioning Success

- [x] Database created and accessible
- [x] Least-privilege user created
- [x] pg_stat_statements enabled
- [x] Autovacuum tuned
- [x] Credentials saved securely
- [x] Monitoring queries generated

### Dataset Quality Success

- [x] Heavy rooms present (top 1% has 15%+ of messages)
- [x] Power-law tail realistic (p99/p50 > 10x)
- [x] Type variety present
- [x] Diurnal patterns present
- [x] No PII detected
- [x] Referential integrity intact
- [x] Timestamps ordered correctly
- [x] All tests pass
- [x] Ready for Phase 2 testing

---

**Status**: âœ… COMPLETE  
**Created**: 2025-10-22  
**Next Step**: Run validation on current 500K message dataset
